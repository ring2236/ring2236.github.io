---
layout: post
title: "多层感知机学习"
date:   2025-04-21
tags: [机器学习]
comments: true
author: kexin
mathjax : true
---
参考：课堂&李沐&GPT
<!-- more -->
# 感知机
![来自李沐老师的课件（感知机原理）](https://ring2236.github.io/images/0421/Perception_1.png "感知机")

$/alpha$

# 多层感知机
```python
import torch
from torch import nn, optim
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler # 对特征做零均值、单位方差的标准化
from torch.utils.data import TensorDataset, DataLoader # 把 NumPy 数据转成 PyTorch 可迭代的批次

# 1. 生成并划分数据
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42) # 生成 1000 个点，噪声强度 0.2，标签 y∈{0,1}。这些点在二维平面上呈两个月牙形，适合测试非线性可分分类器。
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 2. 标准化
# 标化的输入输出是什么？
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val   = scaler.transform(X_val)

# 3. 转为 TensorDataset/DataLoader
train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                         torch.tensor(y_train, dtype=torch.long))
                         # 把 NumPy 数组封装成 (feature, label) 对
val_ds   = TensorDataset(torch.tensor(X_val,   dtype=torch.float32),
                         torch.tensor(y_val,   dtype=torch.long))

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
# 每次从 train_ds 中随机抽取 32 个样本（shuffle=True），组成一个 batch。
val_loader   = DataLoader(val_ds,   batch_size=32)

# 4. 定义 MLP 模型
# 两层：输入，隐藏，输出
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 16),   # 输入维度 2 → 隐藏层 16
            nn.ReLU(),
            nn.Linear(16, 16),  # 隐藏层 16 → 隐藏层 16
            nn.ReLU(),
            nn.Linear(16, 2)    # 隐藏层 16 → 输出 2 类
        )
    def forward(self, x):
        return self.net(x)
        

model = MLP()

# 5. 损失函数与优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1) # 一阶梯度下降

# 6. 训练与验证函数
def train_epoch():
    model.train() # 切换到训练模式（会启用 batchnorm、dropout 等）
    total_loss = 0
    for xb, yb in train_loader:
        preds = model(xb) # 前向计算 logits s
        loss = criterion(preds, yb) # 计算批次损失
        optimizer.zero_grad() # 清零上一步梯度
        loss.backward()  # 反向传播，计算 ∂L/∂θ
        optimizer.step() # 更新参数
        total_loss += loss.item()
    return total_loss / len(train_loader) # 返回平均损失

def evaluate():
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for xb, yb in val_loader:
            preds = model(xb).argmax(dim=1)
            correct += (preds == yb).sum().item()
            total   += yb.size(0)
    return correct / total

# 7. 运行训练
for epoch in range(1, 21): #循环 20 个 epoch，不断用训练集更新模型，然后在验证集上测试性能
    loss = train_epoch()
    acc  = evaluate()
    print(f"Epoch {epoch:02d}: loss={loss:.4f}, val_acc={acc:.4f}")



```
